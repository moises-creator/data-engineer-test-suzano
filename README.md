# ğŸ›  Data Engineer Test Solution

## ğŸš€ Projeto: AutomaÃ§Ã£o de Coleta e Armazenamento de Dados EconÃ´micos

Este projeto tem como objetivo criar um pipeline para coletar, processar e armazenar dados econÃ´micos a partir do site **Investing.com**, utilizando ferramentas modernas de orquestraÃ§Ã£o, containerizaÃ§Ã£o e computaÃ§Ã£o em nuvem.

---

## ğŸ“‹ **DescriÃ§Ã£o do Problema**

### Dados Requeridos:
1. **Chinese Caixin Services Index**:
   - Fonte: [Investing.com](https://br.investing.com/economic-calendar/chinese-caixin-services-pmi-596)
   - PerÃ­odo: Mensal, de 2012 atÃ© o presente.
   - Campos: `date`, `actual_state`, `close`, `forecast`.

2. **Bloomberg Commodity Index**:
   - Fonte: [Investing.com](https://br.investing.com/indices/bloomberg-commodity)
   - PerÃ­odo: Mensal, de 1991 atÃ© o presente.
   - Campos: `date`, `close`, `open`, `high`, `low`, `volume`.

3. **USD/CNY**:
   - Fonte: [Investing.com](https://br.investing.com/currencies/usd-cny)
   - PerÃ­odo: Mensal, de 1991 atÃ© o presente.
   - Campos: `date`, `close`, `open`, `high`, `low`, `volume`.

### Objetivo:
Criar um pipeline para coletar esses dados automaticamente e armazenÃ¡-los em um banco de dados relacional.

---

## ğŸ“‚ **Estrutura do Projeto**

```bash
.
â”œâ”€â”€ dags/                          # CÃ³digo principal do DAG
â”‚   â”œâ”€â”€ dag_investing.py           # OrquestraÃ§Ã£o do pipeline
â”œâ”€â”€ include/                       # Recursos auxiliares
â”‚   â”œâ”€â”€ task_groups/               # Task Groups usados no DAG
â”‚   â”‚   â”œâ”€â”€ create_tables.py       # Task para criar tabelas no SQL
â”‚   â”‚   â”œâ”€â”€ load_bg.py             # Task para carregar dados do Bloomberg
â”‚   â”‚   â”œâ”€â”€ scraper.py             # FunÃ§Ã£o de scraping centralizada
â”‚   â”‚   â”œâ”€â”€ upload_gcs.py          # Task para upload ao Google Cloud Storage
â”‚   â”‚   â”œâ”€â”€ scraping_operations.py # Outras operaÃ§Ãµes de scraping
â”œâ”€â”€ terraform/                     # Arquivos para provisionamento via Terraform
â”‚   â”œâ”€â”€ main.tf                    # ConfiguraÃ§Ã£o principal do Terraform
â”‚   â”œâ”€â”€ variables.tf               # VariÃ¡veis para parametrizaÃ§Ã£o
â”‚   â”œâ”€â”€ terraform.tfvars           # ConfiguraÃ§Ã£o de outputs do Terraform
â”œâ”€â”€ plugins/                       # Plugins personalizados
â”œâ”€â”€ tests/                         # Testes unitÃ¡rios do projeto
â”œâ”€â”€ Dockerfile                     # ConfiguraÃ§Ã£o do container
â”œâ”€â”€ docker-compose.override.yml    # ConfiguraÃ§Ã£o adicional do Docker Compose
â”œâ”€â”€ requirements.txt               # DependÃªncias Python
â”œâ”€â”€ .env                           # VariÃ¡veis sensÃ­veis (nÃ£o versionado)
â””â”€â”€ README.md                      # DocumentaÃ§Ã£o do projeto

```

---

## ğŸ› ï¸ **Tecnologias Utilizadas**

### Linguagem e Ferramentas:
- **Python** ğŸ: Linguagem principal para desenvolvimento.
- **Apache Airflow** ğŸŒ¬ï¸: OrquestraÃ§Ã£o de pipelines de dados.
- **Astro CLI** ğŸš€: Para gerenciamento de Airflow no Docker.
- **Docker** ğŸ³: ContainerizaÃ§Ã£o do ambiente.
- **Terraform** âš™ï¸: Provisionamento de infraestrutura como cÃ³digo.
- **Google Cloud SQL** â˜ï¸: Banco de dados relacional para armazenamento.

### Bibliotecas Python:
- **Selenium** ğŸ–±ï¸: Para automaÃ§Ã£o de scraping web.
- **Requests** ğŸŒ: Para consumo de APIs REST.
- **rich** âœ¨: Para mensagens de console mais amigÃ¡veis.

---

## âš™ï¸ **InstalaÃ§Ã£o e ConfiguraÃ§Ã£o**

### 1. PrÃ©-requisitos:
- **Docker** instalado ([Guia de InstalaÃ§Ã£o](https://docs.docker.com/get-docker/)).
- **Astro CLI** instalado ([Guia de InstalaÃ§Ã£o](https://docs.astronomer.io/astro/cli/install-cli)).
- **Terraform** instalado ([Guia de InstalaÃ§Ã£o](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)).

### 2. ConfiguraÃ§Ã£o do Ambiente:
1. Clone este repositÃ³rio:
   ```bash
   git clone git@github.com:moises-creator/data-engineer-test.git
   cd data-engineer-test
   ```

2. Configure as variÃ¡veis de ambiente:
   Navegue atÃ© o diretÃ³rio `terraform`:
   ```
   cd terraform
   ```

### 3. Inicialize o Terraform:
1. Digite o comando:
   ```bash
   terraform init
   terraform apply
   ```
Obs: Para destruir o projeto, execute o comando `terraform destroy`. Aguarde alguns minutos para a instÃ¢ncia estar completa para execuÃ§Ã£o. 

2. Acesse o Airflow UI:
   - URL: acesse o ip externo da instÃ¢ncia criada pelo terraform, tire o `s` do https e acrescente a porta na frente da url, exemplo: `http://34.123.54.2:8081`
   - UsuÃ¡rio: `admin`
   - Senha: `admin`
3. Acrescente a conexÃ£o com o gcp utilizando a interface `Connections`:
3.1 Project ID: `google_cloud_default`
3.2 Escolha `GoogleBigQuery`
3.3 Copie e cole a chave json criada no service account do IAM responsÃ¡vel pelo projeto no campo `KeyFile JSON`.


4. Execute a DAG `dag_investing` na interface.
```Obs: O site `investing.com` Ã© um pouco pesado para a mÃ¡quina virtual selecionada, caso apresente falhas na primeira e segunda tentativa, execute novamente por gentileza.```

---

5. Por fim, execute o seguinte comando no terminal local para excluir a instÃ¢ncia:
```terraform destroy```

## ğŸ—‚ï¸ **Fluxo do Pipeline**

1. **Cria bucket**:

1. **Scraping dos Dados**:

2. **Carrega os dados no gcs(bucket)**:

3. **Cria dataset no BigQuery**:

4. **Carrega os dados do bucket para o BigQuery**:

---

## ğŸ“ˆ **Consultando os Dados**

ApÃ³s a execuÃ§Ã£o do pipeline, os dados podem ser consultados no banco **Google Big Query** via qualquer ferramenta de consulta SQL.

---

## ğŸ“‹ **Melhorias Futuras**
- âœ… Automatizar a integraÃ§Ã£o com um dashboard.
- âœ… Implementar Data Quality usando **Soda**.
- âœ… Aplicar **dbt** para modelagem dos dados.

--- 

## âœ¨ **Contato**
Em caso de dÃºvidas ou sugestÃµes, entre em contato comigo pelo GitHub ou LinkedIn.

