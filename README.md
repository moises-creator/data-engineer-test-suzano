# ğŸ›  Data Engineer Test Solution

## ğŸš€ Projeto: AutomaÃ§Ã£o de Coleta e Armazenamento de Dados EconÃ´micos

Este projeto tem como objetivo criar um pipeline para coletar, processar e armazenar dados econÃ´micos a partir do site **Investing.com**, utilizando ferramentas modernas de orquestraÃ§Ã£o, containerizaÃ§Ã£o e computaÃ§Ã£o em nuvem.

---

## ğŸ“‹ **DescriÃ§Ã£o do Problema**

### Dados Requeridos:
1. **Chinese Caixin Services Index**:
   - Fonte: [Investing.com](https://br.investing.com/economic-calendar/chinese-caixin-services-pmi-596)
   - PerÃ­odo: Mensal, de 2012 atÃ© o presente.
   - Campos: `date`, `actual_state`, `close`, `forecast`.

2. **Bloomberg Commodity Index**:
   - Fonte: [Investing.com](https://br.investing.com/indices/bloomberg-commodity)
   - PerÃ­odo: Mensal, de 1991 atÃ© o presente.
   - Campos: `date`, `close`, `open`, `high`, `low`, `volume`.

3. **USD/CNY**:
   - Fonte: [Investing.com](https://br.investing.com/currencies/usd-cny)
   - PerÃ­odo: Mensal, de 1991 atÃ© o presente.
   - Campos: `date`, `close`, `open`, `high`, `low`, `volume`.

### Objetivo:
Criar um pipeline para coletar esses dados automaticamente e armazenÃ¡-los em um banco de dados relacional.

---

## ğŸ“‚ **Estrutura do Projeto**

```bash
.
â”œâ”€â”€ dags/                          # CÃ³digo principal do DAG
â”‚   â”œâ”€â”€ scraping_to_cloud_sql.py   # OrquestraÃ§Ã£o do pipeline
â”œâ”€â”€ include/                       # Recursos auxiliares
â”‚   â”œâ”€â”€ sql/                       # Scripts SQL
â”‚   â”‚   â”œâ”€â”€ create_tables.sql      # CriaÃ§Ã£o das tabelas no Cloud SQL
â”‚   â”‚   â”œâ”€â”€ load_bloomberg.sql     # InserÃ§Ã£o de dados do Bloomberg
â”‚   â”‚   â”œâ”€â”€ load_usd_cny.sql       # InserÃ§Ã£o de dados do USD/CNY
â”‚   â”‚   â”œâ”€â”€ load_china_index.sql   # InserÃ§Ã£o de dados do Ã­ndice chinÃªs
â”‚   â”œâ”€â”€ scraping_utils.py          # FunÃ§Ãµes de scraping
â”œâ”€â”€ Dockerfile                     # ConfiguraÃ§Ã£o do container
â”œâ”€â”€ requirements.txt               # DependÃªncias Python
â”œâ”€â”€ .env                           # VariÃ¡veis sensÃ­veis (nÃ£o versionado)
â””â”€â”€ README.md                      # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ› ï¸ **Tecnologias Utilizadas**

### Linguagem e Ferramentas:
- **Python** ğŸ: Linguagem principal para desenvolvimento.
- **Apache Airflow** ğŸŒ¬ï¸: OrquestraÃ§Ã£o de pipelines de dados.
- **Astro CLI** ğŸš€: Para gerenciamento de Airflow no Docker.
- **Docker** ğŸ³: ContainerizaÃ§Ã£o do ambiente.
- **Google Cloud SQL** â˜ï¸: Banco de dados relacional para armazenamento.

### Bibliotecas Python:
- **Selenium** ğŸ–±ï¸: Para automaÃ§Ã£o de scraping web.
- **Requests** ğŸŒ: Para consumo de APIs REST.
- **psycopg2** ğŸ›¢ï¸: Para conexÃ£o ao banco de dados PostgreSQL.
- **python-dotenv** ğŸ”‘: Para gerenciamento de variÃ¡veis sensÃ­veis.
- **rich** âœ¨: Para mensagens de console mais amigÃ¡veis.

---

## âš™ï¸ **InstalaÃ§Ã£o e ConfiguraÃ§Ã£o**

### 1. PrÃ©-requisitos:
- **Docker** instalado ([Guia de InstalaÃ§Ã£o](https://docs.docker.com/get-docker/)).
- **Astro CLI** instalado ([Guia de InstalaÃ§Ã£o](https://docs.astronomer.io/astro/cli/install-cli)).

### 2. ConfiguraÃ§Ã£o do Ambiente:
1. Clone este repositÃ³rio:
   ```bash
   git clone git@github.com:moises-creator/data-engineer-test.git
   cd data-engineer-test
   ```

2. Configure as variÃ¡veis de ambiente:
   Crie um arquivo `.env` na raiz do projeto com o seguinte conteÃºdo:
   ```env
   DB_CONNECTION_ID=pg_default
   ```

3. Instale as dependÃªncias locais (opcional para desenvolvimento):
   ```bash
   pip install -r requirements.txt
   ```

---

### 3. Inicialize o Ambiente Astro CLI:
1. Suba os serviÃ§os com Docker:
   ```bash
   astro dev start
   ```

2. Acesse o Airflow UI:
   - URL: [http://localhost:8080](http://localhost:8080)
   - UsuÃ¡rio: `admin`
   - Senha: `admin`

3. Execute o DAG `scraping_to_cloud_sql` na interface.

---

## ğŸ—‚ï¸ **Fluxo do Pipeline**

1. **Scraping dos Dados**:
   - TrÃªs tarefas paralelas coletam os dados usando **Selenium** e **Requests**.
   - Os dados sÃ£o salvos em arquivos JSON no diretÃ³rio `/tmp`.

2. **CriaÃ§Ã£o de Tabelas**:
   - Um operador (`CloudSQLExecuteQueryOperator`) cria as tabelas no Google Cloud SQL.

3. **Carregamento dos Dados**:
   - Tarefas paralelas inserem os dados nas tabelas respectivas.

4. **OrquestraÃ§Ã£o**:
   - As tarefas sÃ£o orquestradas usando o **chain** do Airflow:
     ```python
     chain(
         scrape_bloomberg_task, 
         scrape_usd_cny_task, 
         scrape_china_index_task,
         create_tables_task,
         load_bloomberg_task, 
         load_usd_cny_task, 
         load_china_index_task,
     )
     ```

---

## ğŸ“ˆ **Consultando os Dados**

ApÃ³s a execuÃ§Ã£o do pipeline, os dados podem ser consultados no banco **Google Cloud SQL** via qualquer ferramenta de consulta SQL, como o DBeaver ou o cliente psql.

---

## ğŸ“‹ **Melhorias Futuras**
- âœ… Implementar monitoramento com **Airflow SLA** para garantir alertas em falhas.
- âœ… Migrar o banco de dados para um ambiente **BigQuery** para anÃ¡lise de dados escalÃ¡vel.
- âœ… Automatizar a integraÃ§Ã£o com um dashboard.
- âœ… Implementar Data Quality usando **Soda**.
- âœ… Aplicar **dbt** para modelagem dos dados.

--- 

## âœ¨ **Contato**
Em caso de dÃºvidas ou sugestÃµes, entre em contato comigo pelo GitHub ou LinkedIn.

